{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edf6ec21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better visualizations\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c1d0b4",
   "metadata": {},
   "source": [
    "## Load Metadata CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7733f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2800 records from /home/zubii/projects/speech-emotion-gradcam/text_analysis/tess_metadata.csv\n",
      "\n",
      "DataFrame shape: (2800, 4)\n",
      "\n",
      "First 5 rows:\n",
      "                                                Path Speaker Emotion  \\\n",
      "0  /content/drive/MyDrive/speech_emotion_project/...     OAF    Fear   \n",
      "1  /content/drive/MyDrive/speech_emotion_project/...     OAF    Fear   \n",
      "2  /content/drive/MyDrive/speech_emotion_project/...     OAF    Fear   \n",
      "3  /content/drive/MyDrive/speech_emotion_project/...     OAF    Fear   \n",
      "4  /content/drive/MyDrive/speech_emotion_project/...     OAF    Fear   \n",
      "\n",
      "              Transcript  \n",
      "0     Say the word back.  \n",
      "1      Say the word bar.  \n",
      "2     Say the word base.  \n",
      "3     Say the word bath.  \n",
      "4  Say the word being...  \n"
     ]
    }
   ],
   "source": [
    "# Load the metadata CSV\n",
    "csv_file = Path(\"/home/zubii/projects/speech-emotion-gradcam/text_analysis/tess_metadata.csv\")\n",
    "\n",
    "if not Path.exists(csv_file):\n",
    "    print(f\"Error: tess_metadata.csv not found!\")\n",
    "else:\n",
    "    df = pd.read_csv(csv_file)\n",
    "    print(f\"Loaded {len(df)} records from {csv_file}\")\n",
    "    print(f\"\\nDataFrame shape: {df.shape}\")\n",
    "    print(f\"\\nFirst 5 rows:\")\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0968386f",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9dea4143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values:\n",
      "Path          0\n",
      "Speaker       0\n",
      "Emotion       0\n",
      "Transcript    0\n",
      "dtype: int64\n",
      "\n",
      "Data types:\n",
      "Path          object\n",
      "Speaker       object\n",
      "Emotion       object\n",
      "Transcript    object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Check for missing data\n",
    "print(\"Missing values:\")\n",
    "print(df.isnull().sum())\n",
    "print(f\"\\nData types:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b7c92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for valid transcripts\n",
    "df_clean = df[df['Transcript'].notna() & (df['Transcript'].str.len() > 0)].copy()\n",
    "print(f\"Total records: {len(df)}\")\n",
    "print(f\"Records with valid transcripts: {len(df_clean)}\")\n",
    "print(f\"Records excluded: {len(df) - len(df_clean)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc9a4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotion distribution\n",
    "emotion_counts = df_clean['Emotion'].value_counts().sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nEmotion Distribution:\")\n",
    "print(emotion_counts)\n",
    "\n",
    "# Visualize emotion distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar chart\n",
    "emotion_counts.plot(kind='bar', ax=ax1, color='steelblue')\n",
    "ax1.set_title('Emotion Distribution in Dataset', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Emotion')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Pie chart\n",
    "ax2.pie(emotion_counts.values, labels=emotion_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "ax2.set_title('Emotion Distribution (Percentage)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e294bf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speaker distribution\n",
    "speaker_counts = df_clean['Speaker'].value_counts()\n",
    "\n",
    "print(\"\\nSpeaker Distribution:\")\n",
    "print(speaker_counts)\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "speaker_counts.plot(kind='bar', ax=ax, color=['#FF6B6B', '#4ECDC4'])\n",
    "ax.set_title('Speaker Distribution in Dataset', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Speaker Type')\n",
    "ax.set_ylabel('Count')\n",
    "ax.tick_params(axis='x', rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dbb5ba",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54aa53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "X = df_clean['Transcript'].values\n",
    "y = df_clean['Emotion'].values\n",
    "\n",
    "print(f\"Total samples: {len(X)}\")\n",
    "print(f\"Unique emotions: {len(set(y))}\")\n",
    "\n",
    "# Split data into train and test sets (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set size: {len(X_train)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6170ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize text using TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "print(\"Vectorizing transcripts using TF-IDF...\")\n",
    "vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "print(f\"✓ Vectorization complete\")\n",
    "print(f\"Number of features: {X_train_vec.shape[1]}\")\n",
    "print(f\"Training matrix shape: {X_train_vec.shape}\")\n",
    "print(f\"Test matrix shape: {X_test_vec.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec927cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Naive Bayes classifier\n",
    "print(\"Training Naive Bayes classifier...\")\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_vec, y_train)\n",
    "print(\"✓ Model training complete\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_vec)\n",
    "y_pred_proba = model.predict_proba(X_test_vec)\n",
    "print(\"✓ Predictions made\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc65eb3",
   "metadata": {},
   "source": [
    "## Results & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8d8783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "unique_emotions = len(set(y))\n",
    "baseline_accuracy = 1.0 / unique_emotions\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CLASSIFICATION RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results_summary = pd.DataFrame({\n",
    "    'Metric': ['Overall Accuracy', 'Baseline Accuracy (Random Guessing)', 'Improvement over Baseline', 'Number of Emotions'],\n",
    "    'Value': [\n",
    "        f\"{accuracy*100:.2f}%\",\n",
    "        f\"{baseline_accuracy*100:.2f}%\",\n",
    "        f\"{(accuracy - baseline_accuracy)*100:.2f}%\",\n",
    "        str(unique_emotions)\n",
    "    ]\n",
    "})\n",
    "\n",
    "display(results_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf70328d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Accuracy Comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "accuracies = [accuracy, baseline_accuracy]\n",
    "labels = ['Text Model\\nAccuracy', 'Baseline\\n(Random Guessing)']\n",
    "colors = ['#FF6B6B' if accuracy < 0.5 else '#4ECDC4', '#95E1D3']\n",
    "\n",
    "bars = ax.bar(labels, accuracies, color=colors, width=0.6, edgecolor='black', linewidth=2)\n",
    "\n",
    "# Add percentage labels on bars\n",
    "for i, (bar, acc) in enumerate(zip(bars, accuracies)):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{acc*100:.2f}%',\n",
    "            ha='center', va='bottom', fontsize=14, fontweight='bold')\n",
    "\n",
    "ax.set_ylim(0, 1.0)\n",
    "ax.set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Text-Only Model Performance vs Baseline', fontsize=14, fontweight='bold')\n",
    "ax.axhline(y=baseline_accuracy, color='red', linestyle='--', linewidth=2, alpha=0.7, label='Baseline')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "if accuracy <= baseline_accuracy:\n",
    "    print(\"\\n⚠️  WARNING: Model performance is AT OR BELOW baseline!\")\n",
    "    print(\"This shows that text transcripts alone provide little to no predictive value for emotion recognition.\")\n",
    "else:\n",
    "    print(f\"\\n✓ Model performs {(accuracy - baseline_accuracy)*100:.2f}% better than random guessing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcf63ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CLASSIFICATION REPORT\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "report_df = pd.DataFrame(report).transpose()\n",
    "report_df = report_df.round(4)\n",
    "\n",
    "print(\"\\nPer-Class Performance Metrics:\")\n",
    "display(report_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4b1967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "emotions = sorted(set(y))\n",
    "cm = confusion_matrix(y_test, y_pred, labels=emotions)\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm_df = pd.DataFrame(cm, index=emotions, columns=emotions)\n",
    "print(cm_df)\n",
    "\n",
    "# Visualize Confusion Matrix\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues', cbar=True, ax=ax, \n",
    "            cbar_kws={'label': 'Count'}, annot_kws={'size': 11, 'weight': 'bold'})\n",
    "\n",
    "ax.set_title('Confusion Matrix - Text-Based Emotion Classifier', fontsize=14, fontweight='bold', pad=20)\n",
    "ax.set_ylabel('True Emotion', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('Predicted Emotion', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a716215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-emotion accuracy\n",
    "per_emotion_accuracy = {}\n",
    "for emotion in emotions:\n",
    "    mask = y_test == emotion\n",
    "    if mask.sum() > 0:\n",
    "        emotion_accuracy = accuracy_score(y_test[mask], y_pred[mask])\n",
    "        per_emotion_accuracy[emotion] = emotion_accuracy\n",
    "\n",
    "per_emotion_df = pd.DataFrame(list(per_emotion_accuracy.items()), \n",
    "                               columns=['Emotion', 'Accuracy'])\n",
    "per_emotion_df['Accuracy'] = per_emotion_df['Accuracy'].apply(lambda x: f\"{x*100:.2f}%\")\n",
    "per_emotion_df = per_emotion_df.sort_values('Emotion')\n",
    "\n",
    "print(\"\\nPer-Emotion Accuracy:\")\n",
    "display(per_emotion_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3befb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize per-emotion accuracy\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "emotions_list = list(per_emotion_accuracy.keys())\n",
    "accuracies_list = [per_emotion_accuracy[e]*100 for e in emotions_list]\n",
    "\n",
    "bars = ax.bar(emotions_list, accuracies_list, color='steelblue', edgecolor='black', linewidth=1.5)\n",
    "\n",
    "# Color bars based on performance\n",
    "for bar, acc in zip(bars, accuracies_list):\n",
    "    if acc >= 50:\n",
    "        bar.set_color('#4ECDC4')\n",
    "    else:\n",
    "        bar.set_color('#FF6B6B')\n",
    "\n",
    "ax.axhline(y=accuracy*100, color='red', linestyle='--', linewidth=2, label='Overall Accuracy')\n",
    "ax.axhline(y=baseline_accuracy*100, color='orange', linestyle='--', linewidth=2, label='Baseline')\n",
    "\n",
    "ax.set_ylabel('Accuracy (%)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Per-Emotion Classification Accuracy', fontsize=14, fontweight='bold')\n",
    "ax.set_ylim(0, 100)\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "ax.legend(fontsize=10)\n",
    "\n",
    "# Add percentage labels\n",
    "for bar, acc in zip(bars, accuracies_list):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{acc:.1f}%',\n",
    "            ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35baab0",
   "metadata": {},
   "source": [
    "## Key Findings & Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a9090f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"CONCLUSION: WHY TEXT ALONE IS INSUFFICIENT FOR EMOTION RECOGNITION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "findings = f\"\"\"\n",
    "KEY FINDINGS:\n",
    "\n",
    "1. LOW ACCURACY:\n",
    "   - Text-only model achieves only {accuracy*100:.2f}% accuracy\n",
    "   - Baseline (random guessing) achieves {baseline_accuracy*100:.2f}%\n",
    "   - Model improvement: {(accuracy - baseline_accuracy)*100:.2f}% over baseline\n",
    "\n",
    "2. POOR GENERALIZATION:\n",
    "   - High confusion between emotion classes\n",
    "   - Many emotions misclassified\n",
    "   - Words used across different emotions are too similar\n",
    "\n",
    "3. TRANSCRIPTS ARE EMOTION-NEUTRAL:\n",
    "   - \"I hate you\" (angry) vs \"I hate you\" (sad) = identical text\n",
    "   - Emotion is conveyed through ACOUSTIC properties, not words\n",
    "   - The same sentence can express different emotions with different:\n",
    "     * Pitch (fundamental frequency)\n",
    "     * Intensity (loudness/energy)\n",
    "     * Duration (speech rate)\n",
    "     * Voice quality (timbre, jitter, shimmer)\n",
    "\n",
    "SOLUTION:\n",
    "\n",
    "✓ Use ACOUSTIC FEATURES (prosody) for emotion recognition\n",
    "✓ Extract features like:\n",
    "  - Mel-Frequency Cepstral Coefficients (MFCCs)\n",
    "  - Pitch contours\n",
    "  - Energy/intensity patterns\n",
    "  - Spectral features\n",
    "\n",
    "✓ Use deep learning (CNNs, RNNs) on spectrograms or audio features\n",
    "✓ Apply Grad-CAM to visualize which acoustic features matter most\n",
    "\n",
    "IMPLICATION:\n",
    "This analysis justifies the use of Grad-CAM on audio-based models\n",
    "to identify which acoustic features drive emotion predictions!\n",
    "\"\"\"\n",
    "\n",
    "print(findings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f6f2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary Table\n",
    "summary_data = {\n",
    "    'Aspect': [\n",
    "        'Total Dataset Size',\n",
    "        'Valid Transcripts',\n",
    "        'Training Samples',\n",
    "        'Test Samples',\n",
    "        'Unique Emotions',\n",
    "        'Feature Dimension',\n",
    "        'Model Type',\n",
    "        'Text Model Accuracy',\n",
    "        'Baseline Accuracy',\n",
    "        'Improvement',\n",
    "        'Status'\n",
    "    ],\n",
    "    'Value': [\n",
    "        str(len(df)),\n",
    "        str(len(df_clean)),\n",
    "        str(len(X_train)),\n",
    "        str(len(X_test)),\n",
    "        str(unique_emotions),\n",
    "        str(X_train_vec.shape[1]),\n",
    "        'Naive Bayes (TF-IDF)',\n",
    "        f\"{accuracy*100:.2f}%\",\n",
    "        f\"{baseline_accuracy*100:.2f}%\",\n",
    "        f\"{(accuracy - baseline_accuracy)*100:.2f}%\",\n",
    "        '❌ Insufficient' if accuracy < 0.6 else '⚠️  Moderate' if accuracy < 0.8 else '✓ Good'\n",
    "    ]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPERIMENT SUMMARY\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "display(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf77780",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Based on this analysis, the next phase should focus on:\n",
    "\n",
    "1. **Extract Acoustic Features** from audio files\n",
    "   - MFCCs, pitch, energy, spectral features\n",
    "   - Create spectrograms for deep learning\n",
    "\n",
    "2. **Train Audio-Based Model**\n",
    "   - CNN on spectrograms\n",
    "   - LSTM on acoustic sequences\n",
    "   - Hybrid architectures\n",
    "\n",
    "3. **Apply Grad-CAM Visualization**\n",
    "   - Identify which acoustic regions drive predictions\n",
    "   - Understand learned attention patterns\n",
    "   - Provide interpretability to the model\n",
    "\n",
    "4. **Compare Results**\n",
    "   - Text-only: ~{:.0f}% accuracy\n",
    "   - Audio-only: Expected >80% accuracy\n",
    "   - Multi-modal fusion: Expected >90% accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
